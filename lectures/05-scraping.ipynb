{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61fbd0b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# STA 220 Data & Web Technologies for Data Analysis\n",
    "\n",
    "### Lecture 5, 1/21/25, Web Scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e394d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Announcements\n",
    "\n",
    " - HW 1 due this Sunday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44da036",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Last week's topics\n",
    "\n",
    " - Web APIs\n",
    "     - documented APIs\n",
    "     - undocumented APIs\n",
    "     - devtools\n",
    "     - `requests` package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65f0885",
   "metadata": {},
   "source": [
    "### Final Project\n",
    "\n",
    "You should work alone or with up to two partners. The\n",
    "purpose of the project is to provide you with real data aquisition experience,\n",
    "which includes:\n",
    "\n",
    "* Posing questions / finding challenges\n",
    "* Finding data sources\n",
    "* Accessing the data \n",
    "* Processing the data \n",
    "* Exploring and visualizing the data\n",
    "* Presenting your findings through writing\n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "To narrow down your project to just one topic, think\n",
    "about:\n",
    "\n",
    "*   What questions does your topic address or what problems does your topic\n",
    "    solve? Why and to whom are these meaningful?\n",
    "*   What's challenging about your topic?\n",
    "*   Are there credible, **public** datasets available to explore the topic?\n",
    "    See below for some suggested data sources.\n",
    "*   Is a 6-week project long enough to explore the topic reasonably well?\n",
    "\n",
    "As inspiration and an example of what can be done with public datasets, see [I\n",
    "Quant NY][NY]. \n",
    "\n",
    "[NY]: http://iquantny.tumblr.com/post/144197004989/the-nypd-was-systematically-ticketing-legally\n",
    "\n",
    "#### Grading criteria\n",
    "\n",
    "The final report is due on __March 16__. The report should be 8-10 pages\n",
    "including writing and visualizations, but excluding code. \n",
    "\n",
    "We will score your report according to:\n",
    "\n",
    "* Reporting (20%): Are there clear research questions that you asked, and did you\n",
    "    address these in an orderly fashion? Did you make well justified\n",
    "    conclusions? Is your project sensible and easy to read?\n",
    "* Data Aquisition and Processing (50%):  How much work was necessary to get your data,\n",
    "    which includes web APIs, web scraping, and reading data from files. \n",
    "    Did you process the data in an clear, efficient,\n",
    "    and organized way? Do you join multiple data sources appropriately? Did you\n",
    "    work with unstructured data? Did you store your processed data in an\n",
    "    efficient way, using well-thought-out data structures or a database?\n",
    "* Vizualisation (20%): \n",
    "    Do your visualizations follow best practices?\n",
    "    Do they give insight to your project? Are the methods tailored to your\n",
    "    specific topic and data (not generic or off-the-shelf)?\n",
    "* Code (10%): Is your code well-organized and easy to read? Is your code\n",
    "    reproducible? Is your code documented? Is your code reasonably efficient?\n",
    "    Did you use appropriate data structures and algorithms?\n",
    "\n",
    "Grading scales:\n",
    "\n",
    "Grade            | Points\n",
    "------------     | -------\n",
    "Good             | 10\n",
    "Satisfactory     | 8\n",
    "Poor             | 6\n",
    "Partial Work     | 4\n",
    "No Work          | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244d018",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Today's topics\n",
    "\n",
    " - Scraping Tables with `pandas`\n",
    " - HTML\n",
    " - XML\n",
    " - Parser\n",
    " - Extracting Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1639026",
   "metadata": {},
   "source": [
    "### Ressources\n",
    "\n",
    "* [`requests` documentation](http://docs.python-requests.org/en/master/)\n",
    "* [`requests-html` documentation](https://html.python-requests.org/)\n",
    "* [W3 Schools](https://www.w3schools.com/html/default.asp)\n",
    "* [MDN HTML Reference](https://developer.mozilla.org/en-US/docs/Web/HTML/Element)\n",
    "* [XPath Diner](http://www.topswagcode.com/xpath/) - an interactive XPath tutorial\n",
    "* [CSS Diner](https://flukeout.github.io/) - an interactive CSS Selector tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0c29be",
   "metadata": {},
   "source": [
    "### Scraping Tables with `pandas`\n",
    "\n",
    "For data in a `table` element, we can use __Pandas__ instead of writing a scraper. \n",
    "\n",
    "Wikipedia provides lots of useful information in tables. Let's get the Wikipedia list of [US cities by area][wiki].\n",
    "\n",
    "[wiki]: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b508f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de307052",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca22846",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e53c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c79f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = tabs[1]\n",
    "tbl.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03deadac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c40a96",
   "metadata": {},
   "source": [
    "To process this information, unusable items have to be removed. We are going to do that with `regex`. We will learn more about `regex` later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406dff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub \n",
    "def remove(string):\n",
    "    '''\n",
    "    Removes everything inside [], a whitespace before that and *'s.\n",
    "    '''\n",
    "    if isinstance(string, str):\n",
    "        string = sub(r'\\s*\\[.*\\]\\**', '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d55e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl['City'].iloc[4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d67468",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove(tbl['City'].iloc[4][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23e9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove(1706.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408948cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.columns = [remove(i) for i in tbl.columns] # remove from table columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a3e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl = tbl.applymap(remove) #remove from all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5320fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0b8964",
   "metadata": {},
   "outputs": [],
   "source": [
    "tbl.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b669944",
   "metadata": {},
   "source": [
    "### HTML\n",
    "\n",
    "Web pages are written in _hypertext markup language_ (HTML). HTML files (`.htm` or `.html`) are plain text, just like JSON, Python scripts, and R scripts.\n",
    "\n",
    "In HTML, we use _tags_ to create _elements_ of a web page. Elements add formatting and structure to the page.\n",
    "\n",
    "* Tags usually come in pairs: an opening tag and a closing tag.\n",
    "* Tags are written `<NAME>` for opening tags, `</NAME>` for closing tags, and `<NAME />` for singleton tags.\n",
    "* Opening and singleton tags can have _attributes_ that contain additional information. Attributes are written `ATTRIBUTE=VALUE` after the tag name. \n",
    "\n",
    "See [here](https://developer.mozilla.org/en-US/docs/Learn/Getting_started_with_the_web/HTML_basics) for a more detailed explanation, and [here](https://developer.mozilla.org/en-US/docs/Web/HTML/Element) for a list of valid HTML elements.\n",
    "\n",
    "#### Example\n",
    "\n",
    "[wiki]: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area\n",
    "\n",
    "From now on, we will use an artificial an example:\n",
    "\n",
    "```html\n",
    "<p>This page is famous and this <b>word</b> is emphasized.</p>\n",
    "```\n",
    "\n",
    "```html\n",
    "<p>This <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">page</a> is famous and this <strong>word</strong> is emphasized.</p>\n",
    "```\n",
    "\n",
    "```html\n",
    "<li>1. Something</li>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9dd8be",
   "metadata": {},
   "source": [
    "<p>This page is famous and this <b>word</b> is emphasized.</p>\n",
    "<p>This <a href=\"https://www.youtube.com/watch?v=dQw4w9WgXcQ\">page</a> is famous and this <strong>word</strong> is emphasized.</p>\n",
    "<li>1. Something</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47824451",
   "metadata": {},
   "source": [
    "The `p` tag marks a paragraph, the `a` tag marks a link (an _anchor_), the `strong` tag marks emphasized text,\n",
    "and `li` tag marks a list.\n",
    "\n",
    "Here's a string that contains HTML for a simple, complete website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = \"\"\"\n",
    "<html>\n",
    "<head>\n",
    "    <title>This is the Title!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <p>This is a paragraph!</p>\n",
    "    <p id=\"best-paragraph\">This is another paragraph! &#127790; and another anchor <a href=\"https://pudding.cool\"> asdf</a> </p>\n",
    "    <p>Visit <a href=\"https://pudding.cool\"> The Pudding</a>.</p>\n",
    "    <span>This is a span, it comes with an taco &#127790;</span>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2d63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a609d94",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "    <title>This is the Title!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <p>This is a paragraph!</p>\n",
    "    <p id=\"best-paragraph\">This is another paragraph! &#127790;</p>\n",
    "    <p>Visit <a href=\"https://pudding.cool\">The Pudding</a>.</p>\n",
    "    <span>This is a span, it comes with an taco &#127790;</span>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e19ff",
   "metadata": {},
   "source": [
    "The `<span>` tag is an inline container used to mark up a part of a text, or a part of a document.\n",
    "    \n",
    "For example, you can write the code\n",
    "```\n",
    "<p>My hat is <span style=\"color:blue\">blue</span>.</p>    \n",
    "```  \n",
    "    \n",
    "<p>My hat is <span style=\"color:blue\">blue</span>.</p>     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce6c7c3",
   "metadata": {},
   "source": [
    "### XML\n",
    "\n",
    "_Extensible markup language_ (XML) also uses tags to create elements. We say XML is _extensible_ because you can create your own XML elements (unlike HTML). People typically use XML to describe structure and meaning of data, rather than for formatting.\n",
    "\n",
    "We'll use the same process to extract data from both HTML and XML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52392d7c",
   "metadata": {},
   "source": [
    "### Parser\n",
    "\n",
    "A _parser_ converts formatted data into familiar data structures. We've used __requests__' built-in JSON parser, but the package doesn't have a built-in HTML/XML parser. Fortunately, there are many other Python packages for parsing HTML/XML and web scraping.\n",
    "\n",
    "HTML/XML Parsers:\n",
    "* [lxml](https://lxml.de/)\n",
    "* [html5lib](https://github.com/html5lib/html5lib-python)\n",
    "* [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/)\n",
    "* [requests-html](https://docs.python-requests.org/projects/requests-html/en/latest/)\n",
    "\n",
    "Scraper Frameworks (_convenient after learning the basics with parsers_):\n",
    "* [scrapy](https://scrapy.org/)\n",
    "* [newspaper3k](https://github.com/codelucas/newspaper)\n",
    "\n",
    "Even more [here](https://github.com/lorien/awesome-web-scraping/blob/master/python.md#web-scraping-frameworks).\n",
    "\n",
    "We'll use __lxml__ here (check the [doc](https://lxml.de/apidoc/index.html)), but you're welcome to use other packages on assignments and the project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10396db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lxml.html as lx\n",
    "\n",
    "html = lx.fromstring(page)\n",
    "html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d359a563",
   "metadata": {},
   "source": [
    "<html>\n",
    "<head>\n",
    "    <title>This is the Title!</title>\n",
    "</head>\n",
    "\n",
    "<body>\n",
    "    <p>This is a paragraph!</p>\n",
    "    <p id=\"best-paragraph\">This is another paragraph! &#127790; and another anchor <a href=\"https://pudding.cool\"> asdf</a> </p>\n",
    "    <p>Visit <a href=\"https://pudding.cool\"> The Pudding</a>.</p>\n",
    "    <span>This is a span, it comes with an taco &#127790;</span>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbde72d",
   "metadata": {},
   "source": [
    "#### Finding Elements\n",
    "\n",
    "Elements are nested, so an HTML document is like a tree:\n",
    "```\n",
    "html\n",
    "├── head\n",
    "│   └── title\n",
    "└── body\n",
    "    ├── p\n",
    "    ├── p\n",
    "    ├── p\n",
    "    │   └── a\n",
    "    └── span\n",
    "```\n",
    "This is similar to the file system on your computer. The key difference is that elements at the same level can have the same tag name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96f27c",
   "metadata": {},
   "source": [
    "#### XPath\n",
    "\n",
    "The _XML Path Language_ (XPath) lets us write paths to elements. XPath paths look a lot like file paths. XPath is not Python-specific!\n",
    "\n",
    "The `.xpath()` method gets all elements at an XPath path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7334818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath(\"/html/head/title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ae698",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath(\"//p/a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085401cb",
   "metadata": {},
   "source": [
    "Since there may be more than one element, the method always returns a list.\n",
    "\n",
    "Absolute paths are not robust for scraping. An update to a web page that adds a single tag can break a scraper that uses absolute paths. In XPath, `//` means \"anywhere below\". We'll use `//` often because it's more robust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f8942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath(\"//a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3569c64e",
   "metadata": {},
   "source": [
    "What if we just elements want that satisfy a certain condition? In XPath, `[ ]` filters out elements that don't match a condition. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd4c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath(\"//p[@id = 'best-paragraph']/a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c9fbae",
   "metadata": {},
   "source": [
    "[XPath Diner](http://www.topswagcode.com/xpath/) is an interactive tutorial that teaches most of the XPath syntax. It takes about 20-60 minutes. Work through it to become an XPath ninja! \n",
    "\n",
    "You can copy the absolute path of a tag from the developer tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811e285",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/html/body/div[2]/div/div[3]/main/div[3]/div[4]/div[1]/div[5]'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b60060",
   "metadata": {},
   "source": [
    "#### CSS Selectors\n",
    "\n",
    "_Cascading Style Sheets_ (CSS) is another language for formatting elements in an HTML document. CSS provides another way to select elements, called _CSS selectors_.\n",
    "\n",
    "CSS selectors are more concise but less flexible than XPath paths. The `.cssselect()` method gets all elements at a CSS selector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94e31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.cssselect(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c96ced",
   "metadata": {},
   "source": [
    "Check out the [CSS Diner](https://flukeout.github.io/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d9f6a5",
   "metadata": {},
   "source": [
    "### Extracting Text and Attributes\n",
    "\n",
    "There are two ways to get text from an element:\n",
    "\n",
    "* `.text` gives text inside the element, but not its children\n",
    "* `.text_content()` gives text inside the element and its children, with all tags removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e24bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41987aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3a2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = html.xpath(\"//a\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2c11af",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaebcbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a2d6ec",
   "metadata": {},
   "source": [
    "We can get values from attributes on an element with `.attrib`, which is a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f525a592",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.attrib[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22048c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.attrib[\"href\"] for x in html.xpath(\"//a\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac98b2",
   "metadata": {},
   "source": [
    "### Writing Scrapers\n",
    "\n",
    "Lets scrape the wiki table ourselves. Attention: We are using request, so pay attention to the file that is being returned. Check on devtools the html element for `<thead>` and see what is returned in the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d813caab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "result = requests.get(url = 'https://en.wikipedia.org/wiki/List_of_United_States_cities_by_area')\n",
    "html = lx.fromstring(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da87449",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = html.xpath('//table')\n",
    "table = tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d851432b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table.text_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ebc1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//thead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda15eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.xpath('//table[2]/tbody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b232f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_rows(html): \n",
    "    rows = html.xpath('//table[2]/tbody/tr')\n",
    "    cells = []\n",
    "    for row in rows: \n",
    "        # ./td|th means we start at the node (not searching the whole doc again), and choose td OR th children\n",
    "        cells.append([cell.text_content() for cell in row.xpath('./td|th')]) # no text, as some cells are in <b>\n",
    "    return cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5c6a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retrieve_rows(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b15b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(retrieve_rows(html))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3965fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.iloc[0]\n",
    "df = df.drop(index = range(2))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441e8abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub \n",
    "def remove(string):\n",
    "    '''\n",
    "    Removes everything inside [], a whitespace before that and *'s.\n",
    "    '''\n",
    "    if isinstance(string, str):\n",
    "        string = sub(r'\\s*\\[.*\\]\\**|\\n|,', '', string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83483172",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [remove(i) for i in df.columns] # remove from table columns\n",
    "df = df.applymap(remove) #remove from all rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86aaa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c789cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns[2:]: #only those cols with vals\n",
    "    df[col] = df[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c13e723",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757d61e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab7739",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "- HTML pages are set up like a filesystem\n",
    "- use `lxml` to parse them in Python\n",
    "- navigate though HTML via xpath or css"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
