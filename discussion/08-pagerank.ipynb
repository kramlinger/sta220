{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efcc1e10",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "\n",
    "We will compute the [PageRank](https://en.wikipedia.org/wiki/PageRank) of the articles of the [Hawaiian](https://en.wikipedia.org/wiki/Hawaiian_language) wikipedia, which is available at [haw.wikipedia.org](https://haw.wikipedia.org/wiki/Ka_papa_kinohi). Additional information of the Hawaiian wiki can be found [here](https://meta.wikimedia.org/wiki/List_of_Wikipedias). \n",
    "\n",
    "_Hints: If you don't speak Hawaiian, you might want to learn the wiki logic from the English wikipedia, and translate your findings. Also, caching is recommended._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e472d",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(a)__ Use the special [AllPages](https://haw.wikipedia.org/wiki/Papa_nui:AllPages) page and understand its logic to retrieve the url of all articles in the Hawaiian wikipedia. Make sure to skip redirections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cd4309",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# a) \n",
    "\n",
    "import requests\n",
    "import requests_cache\n",
    "import lxml.html as lx\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70fc79c",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# Papa_nui:AllPages is retrived from changing over from the english version\n",
    "url = '/w/index.php?title=Papa_nui:AllPages&from='\n",
    "session = requests_cache.CachedSession('./source/disc088')\n",
    "articles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee991bc9",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "while True: \n",
    "    result = session.get('https://haw.wikipedia.org' + url)\n",
    "    if result.raise_for_status(): break    \n",
    "    html = lx.fromstring(result.text)\n",
    "    # ignore all entries in the index that are redirections\n",
    "    # the class attributes are found via inspecting the html\n",
    "    articles.extend(html.xpath('//ul[@class=\"mw-allpages-chunk\"]/li[not(@class=\"allpagesredirect\")]/a/@href'))\n",
    "\n",
    "    try: \n",
    "        # Mea aʻe means Next Page\n",
    "        url = html.xpath('//div[@class=\"mw-allpages-nav\"]/a[contains(text(), \"Mea aʻe\")]/@href')[0]\n",
    "    except: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb057e6",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "len(articles) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95adaa",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(b, i)__ Write a function that scans an article given by its url and retrieves all links to other articles in the Hawaiian wikipedia. Avoid links to special pages, images or the ones that point to another website. Only count the proper article for links that point to a specific section. Use regular expressions to manage these cases. \n",
    "__(ii)__ Make sure to match redirections to their correct destiation article. To this end, find how wikipedia treats redirections and retrieve the true article. _(Help: Try searching for 'uc davis' on en.wikipedia.org')_\n",
    " I used the collection of article urls obtained in (a), which I stored in a dict object to allow for fast lookups. Then, for each new found link I checked whether that link appeared in the dict. If not, It might be a re-direction and receive special attention.  \n",
    "__(iii)__ Request all articles and obtain all links to other articles. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad59ed",
   "metadata": {
    "scrolled": false,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# (b,i)\n",
    "\n",
    "def fetch_links(article):\n",
    "    session = get_session()\n",
    "    result = session.get('https://haw.wikipedia.org' + article)\n",
    "    try: \n",
    "        result.raise_for_status()\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "    html = lx.fromstring(result.text)\n",
    "    links = html.xpath('//div[@id=\"bodyContent\"]//a/@href')\n",
    "    \n",
    "    # match all that are not preceded by 'org', ...\n",
    "    # contain a '/wiki/' term, ...\n",
    "    # and the term after if that does not contain a colon\n",
    "    # and only match the parts preceding a within-page reference (#)\n",
    "    links = [re.findall('(?<!org)\\/wiki\\/(?!.*:)[^#]*', link) for link in links]\n",
    "    links = [link[0] for link in links if link != []] # remove unmatched links\n",
    "\n",
    "    return set(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6ec9a",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# (ii)\n",
    "lookup = {key: value for value, key in enumerate(articles)}\n",
    "\n",
    "def catch_redirect(link):\n",
    "    if lookup.get(link, None) is None: # redirect must have taken place, or link doesn't exist\n",
    "        name = re.findall('(?<=\\/wiki\\/).*', link)[0]\n",
    "        url = 'https://haw.wikipedia.org/w/index.php?title=' + name + '&redirect=no' # this is how wiki treats redirects\n",
    "        result = requests.get(url)\n",
    "        html = lx.fromstring(result.text)\n",
    "        \n",
    "        try: link = html.xpath('//ul[@class=\"redirectText\"]//a/@href|//span[@class=\"mw-redirectedfrom\"]//a/@href')[0]\n",
    "        except: link\n",
    "        \n",
    "        #remove all within-page references\n",
    "        link = re.findall('(?<!org)\\/wiki\\/(?!.*:)[^#]*', link)\n",
    "        if link != []: link = link[0]\n",
    "        else: link = None\n",
    "        \n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefec45e",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# (iii)\n",
    "\n",
    "import concurrent.futures, threading\n",
    "\n",
    "thread_local = threading.local() # instantiates thread to create local data (here: session-attr.)\n",
    "\n",
    "def get_session():\n",
    "    if not hasattr(thread_local, \"session\"): \n",
    "        thread_local.session = requests.Session()\n",
    "    return thread_local.session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ce3f7",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def download_site(article):\n",
    "    session = get_session()\n",
    "    article_id = lookup.get(article)\n",
    "    links = fetch_links(article)\n",
    "    if links is None: pairs = []\n",
    "    elif links == []: pairs = []\n",
    "    else:\n",
    "        links = [catch_redirect(link) for link in links]\n",
    "        pairs = [(article_id, lookup.get(link)) for link in links if lookup.get(link) is not None]\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c9c9b",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def download_all_sites(sites):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = 4) as executor:\n",
    "        results = executor.map(download_site, sites)\n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8a534",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "result = download_all_sites(articles)\n",
    "\n",
    "pairs = []\n",
    "for i in result: pairs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac6720",
   "metadata": {
    "scrolled": true,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "print(sum([len(p) for p in pairs])) # 7021 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf5280",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(c)__ Compute the transition matrix (see [here](https://en.wikipedia.org/wiki/Google_matrix) and [here](https://www.amsi.org.au/teacher_modules/pdfs/Maths_delivers/Pagerank5.pdf) for step-by-step instructions). Make sure to tread dangling nodes. You may want to use: \n",
    "```\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057fdd45",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# (c)\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# \n",
    "n = len(articles)\n",
    "row, col, data = zip(*((row, col, 1 / len(p)) for p in pairs for row, col in p))\n",
    "H = csr_matrix((data, (row, col)), shape = (n, n))\n",
    "\n",
    "dangling = [p==[] for p in pairs]\n",
    "H[dangling,:] = 1 / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a68b8",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "__(d, i)__ Set the damping factor to `0.85` and compute the PageRank for each article, using fourty iterations and starting with a vector with equal entries. __(ii)__ Obtain the top ten articles in terms of PageRank, and, retrieving the articles again, find the correponding English article, if available. \n",
    "\n",
    "_Return the corresponding English article titles of the top ten articles from the Hawaiian wikipedia._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b61249",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# (d, i)\n",
    "\n",
    "vH = lambda v: 0.85 * (v @ H) + 0.15 * np.mean(v)\n",
    "v = np.array([1 / n] * n)\n",
    "#vold = np.array([1] * n)\n",
    "for _ in range(40):\n",
    "#    vold = v\n",
    "    v = vH(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22b9300",
   "metadata": {
    "tags": [
     "comments"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(n), v, label = \"PageRank\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d6c1d",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pagerank = pd.Series(v).sort_values(ascending = False).head(10)\n",
    "page_id = list(pagerank.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f163f8",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc8bf09",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "top10 = [articles[p] for p in page_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5294763",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "def get_english_article(link): \n",
    "\n",
    "    result = requests.get('https://haw.wikipedia.org' + link)\n",
    "    html = lx.fromstring(result.text)\n",
    "    english = html.xpath('//li[@class=\"interlanguage-link interwiki-en mw-list-item\"]/a/@href')\n",
    "\n",
    "    try: url = english[0]\n",
    "    except: url = None\n",
    "\n",
    "    return url\n",
    "\n",
    "def get_title(url): \n",
    "\n",
    "    result = requests.get(url)\n",
    "    html = lx.fromstring(result.text)\n",
    "    titlelist = html.xpath('//span[@class=\"mw-page-title-main\"]')\n",
    "\n",
    "    try: title = titlelist[0].text\n",
    "    except: title = None\n",
    "\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77b0a00",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "english_urls = [get_english_article(link) for link in top10] # one is None\n",
    "title = [get_title(link) for link in english_urls if link is not None]\n",
    "title\n",
    "\n",
    "#'Spain',\n",
    "#'Castile and León',\n",
    "#'Municipality',\n",
    "#'United States',\n",
    "#'Hawaii',\n",
    "#'List of municipalities in Burgos',\n",
    "#'Capital city',\n",
    "#'Lithuania',\n",
    "#'List of municipalities in Soria'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7f644c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
